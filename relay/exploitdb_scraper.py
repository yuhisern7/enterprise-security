"""
ExploitDB Advanced Scraper & AI Learning Module

Actively crawls ExploitDB to:
1. Download exploit code and descriptions
2. Parse attack patterns and signatures
3. Train AI models on real-world exploits
4. Build custom attack signature database
5. Continuously update with new exploits

This makes your AI learn from 50,000+ real exploits.
"""

import requests
import re
import json
import time
import os
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, List, Optional, Any
from collections import defaultdict
import threading


class ExploitDBScraper:
    """Advanced ExploitDB scraper with AI learning capabilities"""
    
    def __init__(self, exploitdb_path: str = "exploitdb", learning_mode: bool = True):
        self.exploitdb_path = exploitdb_path
        self.learning_mode = learning_mode
        self.exploits_data = []
        self.attack_patterns = defaultdict(list)
        self.signature_database = []
        
        # Attack pattern extractors (regex patterns)
        self.pattern_extractors = {
            "sql_injection": [
                r"(union\s+select|select.*from.*where|insert\s+into|drop\s+table)",
                r"(or\s+1\s*=\s*1|'\s+or\s+'1'\s*=\s*'1)",
                r"(sleep\(\d+\)|benchmark\(|waitfor\s+delay)"
            ],
            "xss": [
                r"(<script[^>]*>.*?</script>|javascript:|onerror=|onload=)",
                r"(<img[^>]*onerror|<svg[^>]*onload)",
                r"(alert\(|prompt\(|confirm\()"
            ],
            "command_injection": [
                r"(;|\||\&\&)\s*(cat|ls|wget|curl|nc|bash|sh|powershell)",
                r"(`.*`|\$\(.*\))",
                r"(system\(|exec\(|shell_exec\(|passthru\()"
            ],
            "directory_traversal": [
                r"(\.\.\/|\.\.\\|%2e%2e%2f|%2e%2e\\)",
                r"(/etc/passwd|/etc/shadow|c:\\windows\\system32)",
                r"(file://|php://filter|php://input)"
            ],
            "file_upload": [
                r"(\.php|\.asp|\.jsp|\.exe|\.sh)$",
                r"(Content-Type:\s*image/|multipart/form-data)",
                r"(move_uploaded_file|copy\(|rename\()"
            ],
            "buffer_overflow": [
                r"(\\x[0-9a-f]{2})+",  # Shellcode pattern
                r"(AAAA|%n|%s|%x)",  # Format string
                r"(jmp\s+esp|call\s+esp|pop.*pop.*ret)"
            ],
            "authentication_bypass": [
                r"(admin'--|\'\s+or\s+\'1\'=\'1)",
                r"(logout|logoff|signout).*?(redirect|return)",
                r"(session_destroy|unset\(\$_SESSION)"
            ]
        }
        
        # Statistics
        self.stats = {
            "total_exploits_scraped": 0,
            "patterns_learned": 0,
            "last_scrape": None,
            "attack_types_found": defaultdict(int)
        }
    
    def scrape_exploitdb_full(self, max_exploits: int = 5000) -> int:
        """
        Scrape ExploitDB comprehensively.
        
        Args:
            max_exploits: Maximum number of exploits to scrape (default 5000)
            
        Returns:
            Number of exploits successfully scraped
        """
        print(f"[ExploitDB Scraper] Starting comprehensive scrape (max {max_exploits} exploits)...")
        
        # Check if local database exists
        csv_path = os.path.join(self.exploitdb_path, "files_exploits.csv")
        
        if os.path.exists(csv_path):
            print("[ExploitDB Scraper] Using local ExploitDB database")
            return self._scrape_from_local_db(csv_path, max_exploits)
        else:
            print("[ExploitDB Scraper] Downloading from ExploitDB website")
            return self._scrape_from_web(max_exploits)
    
    def _scrape_from_local_db(self, csv_path: str, max_exploits: int) -> int:
        """Scrape from local ExploitDB CSV database"""
        scraped_count = 0
        
        try:
            with open(csv_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()[1:]  # Skip header
                
                for i, line in enumerate(lines[:max_exploits]):
                    if i % 100 == 0:
                        print(f"[ExploitDB Scraper] Progress: {i}/{min(len(lines), max_exploits)}")
                    
                    try:
                        # CSV format: id,file,description,date,author,type,platform,port
                        parts = line.split(',')
                        
                        if len(parts) >= 6:
                            exploit_id = parts[0].strip()
                            file_path = parts[1].strip()
                            description = parts[2].strip().strip('"')
                            exploit_type = parts[5].strip() if len(parts) > 5 else "unknown"
                            
                            # Read exploit code if available
                            exploit_code = self._read_exploit_file(file_path)
                            
                            # Process exploit
                            exploit_data = {
                                "id": exploit_id,
                                "description": description,
                                "type": exploit_type,
                                "code": exploit_code,
                                "file_path": file_path
                            }
                            
                            # Learn from exploit
                            if self.learning_mode:
                                self._learn_from_exploit(exploit_data)
                            
                            self.exploits_data.append(exploit_data)
                            scraped_count += 1
                    
                    except Exception as e:
                        continue
            
            print(f"[ExploitDB Scraper] ✅ Scraped {scraped_count} exploits from local database")
            self.stats["total_exploits_scraped"] = scraped_count
            self.stats["last_scrape"] = datetime.utcnow().isoformat()
            
            return scraped_count
            
        except Exception as e:
            print(f"[ExploitDB Scraper] Error reading local database: {e}")
            return 0
    
    def _scrape_from_web(self, max_exploits: int) -> int:
        """Scrape directly from ExploitDB website (fallback method)"""
        scraped_count = 0
        base_url = "https://www.exploit-db.com"
        
        try:
            # Download CSV from GitLab
            csv_url = "https://gitlab.com/exploit-database/exploitdb/-/raw/main/files_exploits.csv"
            
            print(f"[ExploitDB Scraper] Downloading CSV from {csv_url}")
            response = requests.get(csv_url, timeout=30)
            
            if response.status_code == 200:
                lines = response.text.split('\n')[1:]  # Skip header
                
                for i, line in enumerate(lines[:max_exploits]):
                    if i % 100 == 0:
                        print(f"[ExploitDB Scraper] Progress: {i}/{min(len(lines), max_exploits)}")
                    
                    try:
                        parts = line.split(',')
                        
                        if len(parts) >= 6:
                            exploit_id = parts[0].strip()
                            description = parts[2].strip().strip('"')
                            exploit_type = parts[5].strip() if len(parts) > 5 else "unknown"
                            
                            exploit_data = {
                                "id": exploit_id,
                                "description": description,
                                "type": exploit_type,
                                "code": None  # No code available from web scrape
                            }
                            
                            # Learn from exploit
                            if self.learning_mode:
                                self._learn_from_exploit(exploit_data)
                            
                            self.exploits_data.append(exploit_data)
                            scraped_count += 1
                    
                    except Exception as e:
                        continue
                    
                    # Rate limiting
                    if i % 100 == 0:
                        time.sleep(1)
            
            print(f"[ExploitDB Scraper] ✅ Scraped {scraped_count} exploits from web")
            self.stats["total_exploits_scraped"] = scraped_count
            self.stats["last_scrape"] = datetime.utcnow().isoformat()
            
            return scraped_count
            
        except Exception as e:
            print(f"[ExploitDB Scraper] Web scraping error: {e}")
            return 0
    
    def _read_exploit_file(self, file_path: str) -> Optional[str]:
        """Read exploit code from local file"""
        full_path = os.path.join(self.exploitdb_path, file_path)
        
        try:
            if os.path.exists(full_path):
                with open(full_path, 'r', encoding='utf-8', errors='ignore') as f:
                    return f.read()
        except Exception as e:
            pass
        
        return None
    
    def _learn_from_exploit(self, exploit_data: Dict[str, Any]):
        """
        AI Learning: Extract attack patterns from exploit.
        This is where the AI actually LEARNS from real exploits.
        """
        description = exploit_data.get("description", "").lower()
        code = exploit_data.get("code", "")
        exploit_type = exploit_data.get("type", "unknown")
        
        # Extract patterns from description
        for attack_type, patterns in self.pattern_extractors.items():
            for pattern in patterns:
                # Check in description
                if re.search(pattern, description, re.IGNORECASE):
                    self.attack_patterns[attack_type].append({
                        "exploit_id": exploit_data["id"],
                        "pattern": pattern,
                        "source": "description"
                    })
                    self.stats["attack_types_found"][attack_type] += 1
                
                # Check in code (if available)
                if code and re.search(pattern, code, re.IGNORECASE):
                    self.attack_patterns[attack_type].append({
                        "exploit_id": exploit_data["id"],
                        "pattern": pattern,
                        "source": "code",
                        "code_snippet": self._extract_snippet(code, pattern)
                    })
                    self.stats["attack_types_found"][attack_type] += 1
        
        # Create signature for detection
        signature = self._create_detection_signature(exploit_data)
        if signature:
            self.signature_database.append(signature)
            self.stats["patterns_learned"] += 1
    
    def _extract_snippet(self, code: str, pattern: str, context: int = 50) -> str:
        """Extract code snippet around matched pattern"""
        try:
            match = re.search(pattern, code, re.IGNORECASE)
            if match:
                start = max(0, match.start() - context)
                end = min(len(code), match.end() + context)
                return code[start:end]
        except:
            pass
        return ""
    
    def _create_detection_signature(self, exploit_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Create a detection signature from exploit.
        This signature will be used to detect similar attacks in real-time.
        """
        description = exploit_data.get("description", "").lower()
        code = exploit_data.get("code", "")
        
        # Extract key indicators
        indicators = []
        
        # SQL injection indicators
        if "sql" in description or "injection" in description:
            indicators.extend(["select", "union", "insert", "delete", "drop", "update"])
        
        # XSS indicators
        if "xss" in description or "cross-site" in description:
            indicators.extend(["<script", "javascript:", "onerror=", "onload="])
        
        # Command injection indicators
        if "command" in description or "shell" in description:
            indicators.extend([";", "&&", "|", "system(", "exec("])
        
        # RCE indicators
        if "remote code" in description or "rce" in description:
            indicators.extend(["eval(", "assert(", "include(", "require("])
        
        if indicators:
            return {
                "exploit_id": exploit_data["id"],
                "type": exploit_data["type"],
                "indicators": list(set(indicators)),
                "description": exploit_data["description"][:200],
                "severity": self._calculate_severity(exploit_data)
            }
        
        return None
    
    def _calculate_severity(self, exploit_data: Dict[str, Any]) -> str:
        """Calculate exploit severity"""
        description = exploit_data.get("description", "").lower()
        
        critical_keywords = ["remote code execution", "rce", "root", "privilege escalation"]
        high_keywords = ["sql injection", "authentication bypass", "arbitrary file"]
        
        for keyword in critical_keywords:
            if keyword in description:
                return "CRITICAL"
        
        for keyword in high_keywords:
            if keyword in description:
                return "HIGH"
        
        return "MEDIUM"
    
    def export_learned_signatures(self, output_file: str = "ai_signatures/learned_signatures.json"):
        """Export learned signatures to JSON for AI training"""
        output_path = os.path.join(os.path.dirname(__file__), output_file)
        
        export_data = {
            "metadata": {
                "total_exploits": len(self.exploits_data),
                "total_signatures": len(self.signature_database),
                "attack_types": dict(self.stats["attack_types_found"]),
                "scraped_at": self.stats["last_scrape"]
            },
            "signatures": self.signature_database,
            "attack_patterns": {k: len(v) for k, v in self.attack_patterns.items()}
        }
        
        try:
            with open(output_path, 'w') as f:
                json.dump(export_data, f, indent=2)
            
            print(f"[ExploitDB Scraper] ✅ Exported {len(self.signature_database)} signatures to {output_file}")
            return output_path
        except Exception as e:
            print(f"[ExploitDB Scraper] Export failed: {e}")
            return None
    
    def get_signatures_for_attack_type(self, attack_type: str) -> List[Dict[str, Any]]:
        """Get all signatures for specific attack type (for AI training)"""
        return [
            sig for sig in self.signature_database 
            if attack_type.lower() in sig.get("description", "").lower()
        ]
    
    def get_scraper_stats(self) -> Dict[str, Any]:
        """Get scraper statistics"""
        return {
            "total_exploits_scraped": self.stats["total_exploits_scraped"],
            "signatures_learned": len(self.signature_database),
            "attack_patterns_by_type": dict(self.stats["attack_types_found"]),
            "last_scrape": self.stats["last_scrape"],
            "learning_mode": self.learning_mode
        }
    
    def continuous_scrape(self, interval_hours: int = 24):
        """
        Continuously scrape ExploitDB for new exploits.
        Runs in background thread.
        """
        def _scrape_loop():
            while True:
                print(f"[ExploitDB Scraper] Running scheduled scrape...")
                self.scrape_exploitdb_full(max_exploits=1000)
                self.export_learned_signatures()
                print(f"[ExploitDB Scraper] Next scrape in {interval_hours} hours")
                time.sleep(interval_hours * 3600)
        
        thread = threading.Thread(target=_scrape_loop, daemon=True)
        thread.start()
        print(f"[ExploitDB Scraper] Continuous scraping started (every {interval_hours} hours)")


# Global scraper instance
_scraper = None

def get_scraper() -> ExploitDBScraper:
    """Get global scraper instance"""
    global _scraper
    if _scraper is None:
        _scraper = ExploitDBScraper()
    return _scraper


def start_exploitdb_scraper(exploitdb_path: str = "exploitdb", continuous: bool = True):
    """
    Start ExploitDB scraper with AI learning.
    
    Args:
        exploitdb_path: Path to ExploitDB local database
        continuous: Enable continuous scraping every 24 hours
    """
    scraper = get_scraper()
    scraper.exploitdb_path = exploitdb_path
    
    print("[ExploitDB Scraper] Initializing...")
    
    # Initial scrape
    scraper.scrape_exploitdb_full(max_exploits=5000)
    
    # Export learned signatures
    scraper.export_learned_signatures()
    
    # Start continuous scraping if enabled
    if continuous:
        scraper.continuous_scrape(interval_hours=24)
    
    print("[ExploitDB Scraper] ✅ Ready and learning!")
    return scraper


if __name__ == "__main__":
    # Test scraper
    scraper = start_exploitdb_scraper(exploitdb_path="exploitdb", continuous=False)
    
    # Print stats
    stats = scraper.get_scraper_stats()
    print("\n[SCRAPER STATS]")
    print(json.dumps(stats, indent=2))
    
    # Show SQL injection signatures learned
    sql_sigs = scraper.get_signatures_for_attack_type("sql")
    print(f"\n[SQL Injection Signatures Learned]: {len(sql_sigs)}")
    if sql_sigs:
        print(json.dumps(sql_sigs[:3], indent=2))  # Show first 3
