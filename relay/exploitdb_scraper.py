"""
ExploitDB Advanced Scraper & AI Learning Module

Actively crawls ExploitDB to:
1. Scan all exploit folders (linux, windows, php, etc.)
2. Parse attack patterns and signatures from each category
3. Create organized JSON training files by platform/attack type
4. Build comprehensive attack signature database
5. Store ML training materials in relay/ai_training_materials/

This makes your AI learn from 50,000+ real exploits organized by category.
"""

import requests
import re
import json
import time
import os
from bs4 import BeautifulSoup
from datetime import datetime
from typing import Dict, List, Optional, Any
from collections import defaultdict
import threading
from pathlib import Path


class ExploitDBScraper:
    """Advanced ExploitDB scraper with comprehensive folder scanning and ML training file generation"""
    
    def __init__(self, exploitdb_path: str = "ai_training_materials/exploitdb", output_path: str = "ai_training_materials/exploitdb_signatures"):
        self.exploitdb_path = exploitdb_path
        self.output_path = output_path
        self.exploits_data = []
        self.attack_patterns = defaultdict(list)
        self.signature_database = defaultdict(list)  # Organized by category
        
        # Create output directory
        os.makedirs(output_path, exist_ok=True)
        
        # Attack pattern extractors (regex patterns)
        self.pattern_extractors = {
            "sql_injection": [
                r"(union\s+select|select.*from.*where|insert\s+into|drop\s+table)",
                r"(or\s+1\s*=\s*1|'\s+or\s+'1'\s*=\s*'1)",
                r"(sleep\(\d+\)|benchmark\(|waitfor\s+delay)"
            ],
            "xss": [
                r"(<script[^>]*>.*?</script>|javascript:|onerror=|onload=)",
                r"(<img[^>]*onerror|<svg[^>]*onload)",
                r"(alert\(|prompt\(|confirm\()"
            ],
            "command_injection": [
                r"(;|\||\&\&)\s*(cat|ls|wget|curl|nc|bash|sh|powershell)",
                r"(`.*`|\$\(.*\))",
                r"(system\(|exec\(|shell_exec\(|passthru\()"
            ],
            "directory_traversal": [
                r"(\.\.\/|\.\.\\|%2e%2e%2f|%2e%2e\\)",
                r"(/etc/passwd|/etc/shadow|c:\\windows\\system32)",
                r"(file://|php://filter|php://input)"
            ],
            "file_upload": [
                r"(\.php|\.asp|\.jsp|\.exe|\.sh)$",
                r"(Content-Type:\s*image/|multipart/form-data)",
                r"(move_uploaded_file|copy\(|rename\()"
            ],
            "buffer_overflow": [
                r"(\\x[0-9a-f]{2})+",  # Shellcode pattern
                r"(AAAA|%n|%s|%x)",  # Format string
                r"(jmp\s+esp|call\s+esp|pop.*pop.*ret)"
            ],
            "authentication_bypass": [
                r"(admin'--|\'\s+or\s+\'1\'=\'1)",
                r"(logout|logoff|signout).*?(redirect|return)",
                r"(session_destroy|unset\(\$_SESSION)"
            ],
            "remote_code_execution": [
                r"(eval\(|exec\(|system\(|passthru\()",
                r"(<?php|<%|asp:|jsp:)",
                r"(shell_exec|popen|proc_open)"
            ],
            "privilege_escalation": [
                r"(sudo|root|admin|setuid|setgid)",
                r"(chmod\s+777|chown\s+root)",
                r"(SUID|SGID|sticky\s+bit)"
            ]
        }
        
        # Statistics
        self.stats = {
            "total_exploits_scraped": 0,
            "total_folders_scanned": 0,
            "patterns_learned": 0,
            "last_scrape": None,
            "exploits_by_platform": defaultdict(int),
            "exploits_by_type": defaultdict(int),
            "attack_types_found": defaultdict(int)
        }
    
    def scrape_all_folders(self, max_exploits_per_folder: int = 500) -> int:
        """
        Scrape ALL ExploitDB folders comprehensively.
        
        Args:
            max_exploits_per_folder: Maximum exploits to process per folder
            
        Returns:
            Total number of exploits successfully scraped
        """
        print(f"[ExploitDB Scraper] [START] Starting comprehensive folder scan...")
        print(f"[ExploitDB Scraper] Base path: {self.exploitdb_path}")
        
        exploits_dir = os.path.join(self.exploitdb_path, "exploits")
        
        if not os.path.exists(exploits_dir):
            print(f"[ExploitDB Scraper] ❌ Exploits directory not found: {exploits_dir}")
            return 0
        
        # Get all platform/language folders
        folders = [f for f in os.listdir(exploits_dir) if os.path.isdir(os.path.join(exploits_dir, f))]
        print(f"[ExploitDB Scraper] Found {len(folders)} folders to scan")
        print(f"[ExploitDB Scraper] Folders: {', '.join(sorted(folders))}")
        
        total_scraped = 0
        
        for folder in sorted(folders):
            folder_path = os.path.join(exploits_dir, folder)
            print(f"\n[ExploitDB Scraper] [FOLDER] Processing folder: {folder}")
            
            scraped = self._scan_folder(folder, folder_path, max_exploits_per_folder)
            total_scraped += scraped
            self.stats["total_folders_scanned"] += 1
            self.stats["exploits_by_platform"][folder] = scraped
            
            print(f"[ExploitDB Scraper] [OK] {folder}: {scraped} exploits processed")
        
        self.stats["total_exploits_scraped"] = total_scraped
        self.stats["last_scrape"] = datetime.utcnow().isoformat()
        
        print(f"\n[ExploitDB Scraper] [COMPLETE]")
        print(f"[ExploitDB Scraper] Total Exploits: {total_scraped}")
        print(f"[ExploitDB Scraper] Total Folders: {self.stats['total_folders_scanned']}")
        
        return total_scraped
    
    def _scan_folder(self, folder_name: str, folder_path: str, max_exploits: int) -> int:
        """Scan a single folder and process all exploit files"""
        scraped_count = 0
        
        try:
            # Get all files in folder and subfolders
            exploit_files = []
            for root, dirs, files in os.walk(folder_path):
                for file in files:
                    if file.endswith(('.txt', '.py', '.rb', '.c', '.pl', '.php', '.sh', '.js', '.java')):
                        exploit_files.append(os.path.join(root, file))
            
            # Limit to max_exploits
            exploit_files = exploit_files[:max_exploits]
            
            for i, file_path in enumerate(exploit_files):
                if i % 100 == 0 and i > 0:
                    print(f"   Progress: {i}/{len(exploit_files)}")
                
                try:
                    exploit_data = self._process_exploit_file(file_path, folder_name)
                    if exploit_data:
                        self.exploits_data.append(exploit_data)
                        self.signature_database[folder_name].append(exploit_data)
                        scraped_count += 1
                except Exception as e:
                    continue
            
            return scraped_count
            
        except Exception as e:
            print(f"[ExploitDB Scraper] Error scanning folder {folder_name}: {e}")
            return 0
    
    def _process_exploit_file(self, file_path: str, platform: str) -> Optional[Dict[str, Any]]:
        """Process a single exploit file and extract intelligence"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
            
            # Extract metadata from file
            file_name = os.path.basename(file_path)
            exploit_id = file_name.split('.')[0] if file_name[0].isdigit() else "unknown"
            
            # Determine file type
            file_ext = os.path.splitext(file_name)[1]
            
            # Extract description (usually in first few lines)
            description = self._extract_description(content)
            
            # Detect attack type
            attack_type = self._detect_attack_type(content, description)
            
            # Extract patterns
            patterns = self._extract_patterns(content)
            
            # Create exploit data structure
            exploit_data = {
                "exploit_id": exploit_id,
                "file_name": file_name,
                "file_type": file_ext,
                "platform": platform,
                "attack_type": attack_type,
                "description": description[:500],  # Limit description length
                "patterns": patterns,
                "indicators": self._extract_indicators(content, attack_type),
                "severity": self._calculate_severity(content, description),
                "scraped_at": datetime.utcnow().isoformat()
            }
            
            # Update stats
            self.stats["attack_types_found"][attack_type] += 1
            self.stats["exploits_by_type"][attack_type] += 1
            self.stats["patterns_learned"] += len(patterns)
            
            return exploit_data
            
        except Exception as e:
            # Debug: Print first error for troubleshooting
            if self.stats["total_exploits_scraped"] == 0:
                print(f"[DEBUG] First file processing error: {e}")
                print(f"[DEBUG] File: {file_path}")
            return None
    
    def _extract_description(self, content: str) -> str:
        """Extract description from exploit file"""
        lines = content.split('\n')
        description_lines = []
        
        for line in lines[:50]:  # Check first 50 lines
            line = line.strip()
            if line.startswith(('#', '//', '/*', '*', '--', 'Title:', 'Description:')):
                # Remove comment markers
                clean_line = re.sub(r'^[#/*\-\s]+', '', line)
                if len(clean_line) > 10:
                    description_lines.append(clean_line)
            
            if len(description_lines) >= 5:
                break
        
        return ' '.join(description_lines) if description_lines else "No description"
    
    def _detect_attack_type(self, content: str, description: str) -> str:
        """Detect attack type from content and description"""
        content_lower = content.lower()
        desc_lower = description.lower()
        combined = content_lower + " " + desc_lower
        
        # Priority detection (most specific first)
        if "sql" in desc_lower or "injection" in desc_lower:
            return "sql_injection"
        if "xss" in desc_lower or "cross-site" in desc_lower:
            return "xss"
        if "rce" in desc_lower or "remote code execution" in desc_lower:
            return "remote_code_execution"
        if "buffer overflow" in desc_lower or "bof" in desc_lower:
            return "buffer_overflow"
        if "command injection" in desc_lower or "shell" in desc_lower:
            return "command_injection"
        if "directory traversal" in desc_lower or "path traversal" in desc_lower:
            return "directory_traversal"
        if "file upload" in desc_lower or "upload" in desc_lower:
            return "file_upload"
        if "privilege escalation" in desc_lower or "privesc" in desc_lower:
            return "privilege_escalation"
        if "auth" in desc_lower and "bypass" in desc_lower:
            return "authentication_bypass"
        if "denial of service" in desc_lower or "dos" in desc_lower:
            return "dos"
        if "csrf" in desc_lower:
            return "csrf"
        if "xxe" in desc_lower or "xml external entity" in desc_lower:
            return "xxe"
        if "ssrf" in desc_lower:
            return "ssrf"
        if "deserialization" in desc_lower:
            return "deserialization"
        
        return "unknown"
    
    def _extract_patterns(self, content: str) -> List[str]:
        """Extract attack patterns from content"""
        patterns = []
        
        for attack_type, pattern_list in self.pattern_extractors.items():
            for pattern in pattern_list:
                try:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    if matches:
                        # Deduplicate and limit
                        unique_matches = list(set(matches))[:5]
                        patterns.extend([m if isinstance(m, str) else m[0] for m in unique_matches])
                except:
                    continue
        
        return list(set(patterns))[:20]  # Limit to 20 unique patterns
    
    def _extract_indicators(self, content: str, attack_type: str) -> List[str]:
        """Extract IOCs (Indicators of Compromise)"""
        indicators = []
        
        # IP addresses
        ips = re.findall(r'\b(?:\d{1,3}\.){3}\d{1,3}\b', content)
        indicators.extend([f"ip:{ip}" for ip in list(set(ips))[:5]])
        
        # URLs
        urls = re.findall(r'https?://[^\s<>"\']+', content)
        indicators.extend([f"url:{url}" for url in list(set(urls))[:5]])
        
        # File paths
        if attack_type == "directory_traversal":
            paths = re.findall(r'(/[a-zA-Z0-9/_\-\.]+|C:\\[a-zA-Z0-9\\_\-\.]+)', content)
            indicators.extend([f"path:{path}" for path in list(set(paths))[:5]])
        
        # CVE references
        cves = re.findall(r'CVE-\d{4}-\d{4,7}', content, re.IGNORECASE)
        indicators.extend([f"cve:{cve.upper()}" for cve in list(set(cves))])
        
        return indicators[:20]  # Limit to 20 indicators
    
    def _calculate_severity(self, content: str, description: str) -> str:
        """Calculate exploit severity"""
        combined = (content + " " + description).lower()
        
        critical_keywords = ["remote code execution", "rce", "root", "privilege escalation", "authentication bypass"]
        high_keywords = ["sql injection", "arbitrary file", "command injection", "buffer overflow"]
        medium_keywords = ["xss", "csrf", "directory traversal", "file upload"]
        
        for keyword in critical_keywords:
            if keyword in combined:
                return "CRITICAL"
        
        for keyword in high_keywords:
            if keyword in combined:
                return "HIGH"
        
        for keyword in medium_keywords:
            if keyword in combined:
                return "MEDIUM"
        
        return "LOW"
    
    def export_ml_training_files(self):
        """Export comprehensive ML training files organized by platform and attack type"""
        print(f"\n[ExploitDB Scraper] [EXPORT] Exporting ML training files...")
        
        # Export by platform
        for platform, exploits in self.signature_database.items():
            if exploits:
                output_file = os.path.join(self.output_path, f"{platform}_exploits.json")
                self._export_json(output_file, {
                    "platform": platform,
                    "total_exploits": len(exploits),
                    "exploits": exploits
                })
                print(f"   [OK] {platform}: {len(exploits)} exploits -> {output_file}")
        
        # Export by attack type
        by_attack_type = defaultdict(list)
        for exploit in self.exploits_data:
            by_attack_type[exploit["attack_type"]].append(exploit)
        
        attack_types_dir = os.path.join(self.output_path, "by_attack_type")
        os.makedirs(attack_types_dir, exist_ok=True)
        
        for attack_type, exploits in by_attack_type.items():
            if exploits:
                output_file = os.path.join(attack_types_dir, f"{attack_type}.json")
                self._export_json(output_file, {
                    "attack_type": attack_type,
                    "total_exploits": len(exploits),
                    "exploits": exploits
                })
                print(f"   [OK] {attack_type}: {len(exploits)} exploits")
        
        # Export master index
        master_index = {
            "metadata": {
                "total_exploits": self.stats["total_exploits_scraped"],
                "total_folders": self.stats["total_folders_scanned"],
                "total_patterns": self.stats["patterns_learned"],
                "scraped_at": self.stats["last_scrape"]
            },
            "exploits_by_platform": dict(self.stats["exploits_by_platform"]),
            "exploits_by_attack_type": dict(self.stats["exploits_by_type"]),
            "attack_types_found": dict(self.stats["attack_types_found"])
        }
        
        master_file = os.path.join(self.output_path, "master_index.json")
        self._export_json(master_file, master_index)
        print(f"\n   [OK] Master Index -> {master_file}")
        
        # Export learned signatures (for AI detection)
        signatures_file = os.path.join(self.output_path, "..", "ai_signatures", "learned_signatures.json")
        os.makedirs(os.path.dirname(signatures_file), exist_ok=True)
        
        learned_sigs = {
            "metadata": master_index["metadata"],
            "signatures": [
                {
                    "exploit_id": e["exploit_id"],
                    "platform": e["platform"],
                    "attack_type": e["attack_type"],
                    "indicators": e["indicators"],
                    "patterns": e["patterns"][:10],  # Limit patterns
                    "severity": e["severity"]
                }
                for e in self.exploits_data
            ]
        }
        
        self._export_json(signatures_file, learned_sigs)
        print(f"   [OK] AI Signatures -> {signatures_file}")
        
        print(f"\n[ExploitDB Scraper] [SUCCESS] ML training files exported successfully!")
    
    def _export_json(self, file_path: str, data: Any):
        """Export data to JSON file"""
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"[ExploitDB Scraper] ❌ Export failed for {file_path}: {e}")
    
    def get_scraper_stats(self) -> Dict[str, Any]:
        """Get comprehensive scraper statistics"""
        return {
            "total_exploits_scraped": self.stats["total_exploits_scraped"],
            "total_folders_scanned": self.stats["total_folders_scanned"],
            "signatures_learned": self.stats["patterns_learned"],
            "exploits_by_platform": dict(self.stats["exploits_by_platform"]),
            "exploits_by_attack_type": dict(self.stats["exploits_by_type"]),
            "attack_types_found": dict(self.stats["attack_types_found"]),
            "last_scrape": self.stats["last_scrape"]
        }
    
    def continuous_scrape(self, interval_hours: int = 168):  # Weekly
        """
        Continuously scrape ExploitDB for updates.
        Runs in background thread.
        """
        def _scrape_loop():
            while True:
                print(f"[ExploitDB Scraper] Running scheduled scrape...")
                self.scrape_all_folders(max_exploits_per_folder=500)
                self.export_ml_training_files()
                print(f"[ExploitDB Scraper] Next scrape in {interval_hours} hours")
                time.sleep(interval_hours * 3600)
        
        thread = threading.Thread(target=_scrape_loop, daemon=True)
        thread.start()
        print(f"[ExploitDB Scraper] Continuous scraping started (every {interval_hours} hours)")


# Global scraper instance
_scraper = None

def get_scraper() -> ExploitDBScraper:
    """Get global scraper instance"""
    global _scraper
    if _scraper is None:
        _scraper = ExploitDBScraper()
    return _scraper


def start_exploitdb_scraper(exploitdb_path: str = "ai_training_materials/exploitdb", 
                           output_path: str = "ai_training_materials/exploitdb_signatures",
                           continuous: bool = False):
    """
    Start ExploitDB scraper with comprehensive folder scanning.
    
    Args:
        exploitdb_path: Path to ExploitDB local database
        output_path: Path to store ML training files
        continuous: Enable continuous scraping every week
    """
    scraper = ExploitDBScraper(exploitdb_path, output_path)
    
    print("[ExploitDB Scraper] [START] Initializing comprehensive scraper...")
    
    # Scan all folders (set to 50000 to process everything)
    total = scraper.scrape_all_folders(max_exploits_per_folder=50000)
    
    if total > 0:
        # Export ML training files
        scraper.export_ml_training_files()
        
        # Print stats
        stats = scraper.get_scraper_stats()
        print("\n[SCRAPER STATISTICS]")
        print(json.dumps(stats, indent=2))
        
        # Start continuous scraping if enabled
        if continuous:
            scraper.continuous_scrape(interval_hours=168)  # Weekly
        
        print(f"\n[ExploitDB Scraper] [READY] Learned from {total} exploits across {stats['total_folders_scanned']} platforms")
    else:
        print("[ExploitDB Scraper] [WARNING] No exploits found. Check exploitdb_path.")
    
    return scraper


if __name__ == "__main__":
    # Run scraper
    scraper = start_exploitdb_scraper(
        exploitdb_path="ai_training_materials/exploitdb",
        output_path="ai_training_materials/exploitdb_signatures",
        continuous=False
    )
